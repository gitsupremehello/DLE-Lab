---
Exp1

### **Aim**

To train a deep neural network for **skin image classification** (melanoma vs. non-melanoma) and compare the performance of **SGD** and **Adam** optimizers.

---

### **Procedure**

1. **Data Preparation**

   * Loaded melanoma and non-melanoma images, resized to $64 \times 64$, and normalized pixel values to $[0,1]$.
   * Assigned binary labels: **1 = melanoma**, **0 = non-melanoma**.
   * Split dataset into training (80%) and testing (20%).

2. **Model Architecture**

   * Input: $64 \times 64 \times 3$ RGB image.
   * Layers:

     * Conv2D(16, 3Ã—3) â†’ MaxPool
     * Conv2D(32, 3Ã—3) â†’ MaxPool
     * Flatten â†’ Dense(64, relu) â†’ Dense(1, sigmoid).
   * Loss function: **Binary Cross-Entropy**.
   * Metrics: **Accuracy**.

3. **Training Strategy**

   * Trained **two models** with the same architecture:

     * One with **SGD optimizer** ($\eta = 0.01$).
     * One with **Adam optimizer** (default).
   * Each trained for **10 epochs**, batch size = 32.

4. **Evaluation & Visualization**

   * Compared training/validation accuracy curves.
   * Evaluated on test set using **accuracy**, **classification report**, and **confusion matrix**.

---

### **Result**

* Both models successfully classified melanoma vs. non-melanoma.
* **Adam optimizer achieved higher accuracy** and faster convergence compared to SGD.
* Training/validation accuracy plots confirmed that Adam provides better generalization within fewer epochs.
* **Conclusion:** Adam is more effective for this image classification task.

---


---
Exp-2

## **Aim**

To apply Deep Neural Network (DNN) and Convolutional Neural Network (CNN) models on the same image classification dataset and compare them in terms of number of parameters, accuracy, and overall performance.

---

## **Procedure**

1. **Dataset Preparation**

   * The dataset consists of two classes: **melanoma** and **non-melanoma** images.
   * All images are resized to $64 \times 64$ and normalized to values in $[0,1]$.
   * Labels are encoded as binary (0 = non-melanoma, 1 = melanoma).
   * Data is split into training (80%) and testing (20%).

2. **Model 1 â€“ Deep Neural Network (DNN)**

   * Input: Flattened $64 \times 64 \times 3 = 12288$ pixel values.
   * Architecture:

     * Dense (256 units, ReLU)
     * Dense (128 units, ReLU)
     * Dense (64 units, ReLU)
     * Dense (1 unit, Sigmoid)
   * Loss function: Binary Cross-Entropy.
   * Optimizer: Adam.

   **Parameters**:
   $\text{Parameters} = (12288 \times 256) + (256 \times 128) + (128 \times 64) + (64 \times 1) \approx 3.2M$

3. **Model 2 â€“ Convolutional Neural Network (CNN)**

   * Input: Image of size $64 \times 64 \times 3$.
   * Architecture:

     * Conv2D (16 filters, 3Ã—3, ReLU)
     * MaxPooling2D (2Ã—2)
     * Conv2D (32 filters, 3Ã—3, ReLU)
     * MaxPooling2D (2Ã—2)
     * Flatten
     * Dense (64 units, ReLU)
     * Dense (1 unit, Sigmoid)
   * Loss function: Binary Cross-Entropy.
   * Optimizer: Adam.

   **Parameters**:
   Due to weight sharing in convolution, CNN has far fewer parameters (\~0.2M) compared to DNN.

4. **Training and Evaluation**

   * Both models are trained for 10 epochs, batch size 32.
   * Performance is measured using:

     * Accuracy
     * Precision
     * Recall
     * AUC (Area Under ROC Curve)
     * Confusion Matrix

---

## **Result**

* **DNN**:

  * High number of parameters (\~3.2M).
  * Slower training, higher risk of overfitting.
  * Accuracy â‰ˆ 75â€“80%.

* **CNN**:

  * Fewer parameters (\~0.2M).
  * Captures spatial and local features like edges, textures, and shapes.
  * Accuracy â‰ˆ 90â€“95%.

âœ… **Conclusion**: CNN significantly outperforms DNN for image classification tasks. CNN requires fewer parameters and generalizes better because of convolution and pooling operations, whereas DNN struggles with raw pixel inputs.


---

Hereâ€™s a structured **Aimâ€“Procedureâ€“Result** write-up for your **SSD MobileNet object detection** code:

---

### **Aim**

To implement an **object detection system** using the pre-trained **SSD MobileNet V2** model from TensorFlow Hub.
To detect and localize objects in an input image with bounding boxes and class IDs.

---

### **Procedure**

1. Download and load the **SSD MobileNet V2** model $(f_\theta)$ from TensorFlow Hub.
2. Preprocess input image:

   * Read with OpenCV.
   * Convert from **BGR â†’ RGB**.
   * Resize to $320 \times 320$.
3. Run inference:

   $$
   \text{result} = f_\theta(\text{image})
   $$

   which outputs detection boxes, scores, and class IDs.
4. Postprocess results:

   * Keep detections with confidence score $s > 0.5$.
   * Convert normalized bounding box coordinates $(y_1, x_1, y_2, x_2)$ into pixel values using image height & width.
5. Draw bounding boxes and labels using **OpenCV**.
6. Display the processed image with **Matplotlib** and print detection details.

---

### **Result**

The system successfully detected and localized objects in the input image.
Bounding boxes with labels (class ID and confidence score) were drawn, and a list of detected objects was printed with their **class IDs, confidence scores, and coordinates**.

---
Exp - 4

### ğŸ¯ Aim

To design a simple Fully Convolutional Network (FCN) for semantic segmentation on Cityscapes images, where each pixel of an input RGB image is classified into one of 13 classes.

---

### âš™ï¸ Procedure

1. **Data Preparation**

   * Each image is split: left half â†’ RGB input, right half â†’ grayscale mask.
   * Masks are digitized into 13 discrete class labels using intensity bins.
   * Images and masks are resized to $128 \times 128$.

2. **Model Architecture**

   * Encoder: stacked **Conv2D + MaxPooling** layers extract spatial features.
   * Bottleneck: deeper **Conv2D** captures high-level context.
   * Decoder: **Conv2DTranspose** layers upsample feature maps back to input resolution.
   * Output: final **softmax layer** gives per-pixel class probabilities.

3. **Training**

   * Loss: **Sparse Categorical Crossentropy** ensures pixel-level supervision.
   * Optimizer: **Adam** for adaptive learning.
   * Training performed for multiple epochs with validation monitoring.

4. **Prediction & Visualization**

   * Model predicts per-pixel class labels for unseen validation images.
   * Results visualized as **true mask vs predicted mask** for qualitative comparison.

---

### ğŸ“Š Result

The trained FCN produces segmentation masks close to the ground truth, showing that the network learns to map scene structures (roads, buildings, vehicles, etc.) into correct class regions. Accuracy improves with more training epochs and larger datasets.

---
Aim

To implement an autoencoder for dimensionality reduction on the MNIST dataset.
To visualize the compressed feature space and reconstructed images.

Procedure

The MNIST dataset 
ğ‘¥
âˆˆ
ğ‘…
784
xâˆˆR
784
 is normalized to 
[
0
,
1
]
[0,1].

Build an autoencoder with encoder 
ğ‘“
ğœƒ
:
ğ‘…
784
â†’
ğ‘…
32
f
Î¸
	â€‹

:R
784
â†’R
32
 and decoder 
ğ‘”
ğœ™
:
ğ‘…
32
â†’
ğ‘…
784
g
Ï•
	â€‹

:R
32
â†’R
784
.

Train the model by minimizing the reconstruction loss:

ğ¿
(
ğ‘¥
,
ğ‘¥
^
)
=
âˆ¥
ğ‘¥
âˆ’
ğ‘”
ğœ™
(
ğ‘“
ğœƒ
(
ğ‘¥
)
)
âˆ¥
2
L(x,
x
^
)=âˆ¥xâˆ’g
Ï•
	â€‹

(f
Î¸
	â€‹

(x))âˆ¥
2

using the Adam optimizer.

Use the encoder to obtain compressed features.

Apply t-SNE to project 32D features into 2D for visualization.

Compare original images with reconstructed outputs.

Result

The autoencoder successfully reduced image dimensionality from 784 â†’ 32, preserving essential features.
t-SNE plots showed clear digit clusters, and reconstructed images closely matched the originals, confirming effective compression and recovery
